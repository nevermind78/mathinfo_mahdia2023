{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJwOGN57q6i2"
   },
   "source": [
    "## Chapter 6 Linear two-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6AeutTsq6i4"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRAcXdXUq6i4"
   },
   "source": [
    "In the previous Chapter we discussed the fitting of a linear model to a set of input/output points - otherwise known as *linear regression*. In general all sorts of nonlinear phenomena present themselves, and the data they generate - whose input and output share a nonlinear relationship - are poorly modeled using a linear model, thus causing linear regression to perform rather poorly. This naturally leads to the exploration of fitting *nonlinear* functions to data, referred to in general as *nonlinear regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnC3meVbq6i4"
   },
   "source": [
    "In this Chapter we introduce the most popular form of nonlinear regression dealt with in machine learning - called *two-class classification*.  The first thing that distinguishes this kind of problem from the kind of regression we have seen thus far is *the data itself*, and more particularly *its output*: two-class classification datasets have output values that take on *only one of two values* with each value referred to as a *class*.  Common examples of two class classification problems include face and general object detection, with classes consisting of with a face or object versus non-facial/object images, textual sentiment analysis where classes consist of written product reviews ascribing a positive or negative opinion, and automatic diagnosis of medical conditions where classes consist of medical data corresponding to patients who either do or do not have a specific malady.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyUcVdTIq6i5"
   },
   "source": [
    "This subtle difference is important, and spurs the development of new  cost functions that are better-suited to deal with such data.  Moreover - as we describe in this Chapter - these new cost functions are formulated based on a wide array of motivating perspectives - leading to *logistic regression*, the *perceptron*, and *support vector machines* perspectives on two-class classification.  While these perspectives widely differ on the surface they all - as we will see - reduce to virtually the same essential principle for two-class classification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
